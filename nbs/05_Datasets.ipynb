{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ccd72ad-80c6-4fd2-814b-a08d153fa655",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9270a066-9da9-48ca-b0b7-a820fcd445f2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Importing the the relevant libraries\n",
    "\n",
    "(Note MiniAI is from NBDev functionality, it allows us to export code into libraries that we can \n",
    "later use with other notebooks. I still have to install NB Dev and implement this into my code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83920f2f-c119-4b73-aa89-9afdd96e955c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import math,numpy as np,matplotlib.pyplot as plt\n",
    "from operator import itemgetter\n",
    "from itertools import zip_longest\n",
    "import fastcore.all as fc\n",
    "\n",
    "from torch.utils.data import default_collate\n",
    "\n",
    "# from miniai.training import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be5231b0-dd82-4368-9ecb-85cab6ccbace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging,pickle,gzip,os,time,shutil,torch,matplotlib as mpl\n",
    "from pathlib import Path\n",
    "\n",
    "from torch import tensor,nn,optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset,load_dataset_builder\n",
    "\n",
    "import torchvision.transforms.functional as TF\n",
    "# from fastcore.test import test_cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e195d99d-11a6-4e4a-80d4-f5d335bbcdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\n",
    "torch.manual_seed(1)\n",
    "mpl.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce0ec84a-73ef-4428-90af-e8a516a2c0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6f26ce-b9cd-4133-affd-101ec5a0d6d0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Hugging Face datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce05bbd9-6ccf-4610-b8b4-99c883947aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e3babd51b4a432b805c0255a590bc4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.00k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0edbbdb00fae4bb8a11f51270ebf245b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/1.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fashion-MNIST is a dataset of Zalando's article imagesâ€”consisting of a training set of\n",
      "60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image,\n",
      "associated with a label from 10 classes. We intend Fashion-MNIST to serve as a direct drop-in\n",
      "replacement for the original MNIST dataset for benchmarking machine learning algorithms.\n",
      "It shares the same image size and structure of training and testing splits.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "name = 'fashion_mnist'\n",
    "ds_builder = load_dataset_builder(name)\n",
    "print(ds_builder.info.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16c80e75-e1c2-4f8e-871e-5f8982fbffc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': Image(decode=True, id=None),\n",
       " 'label': ClassLabel(num_classes=10, names=['T - shirt / top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'], id=None)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_builder.info.features  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a65cec8b-ef06-4155-bd09-4df7982d563a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': SplitInfo(name='train', num_bytes=31296655, num_examples=60000, dataset_name='fashion_mnist'),\n",
       " 'test': SplitInfo(name='test', num_bytes=5233818, num_examples=10000, dataset_name='fashion_mnist')}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_builder.info.splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d8acbdd-f8a6-48a7-b06a-f4e9a9d67c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset fashion_mnist/fashion_mnist (download: 29.45 MiB, generated: 34.84 MiB, post-processed: Unknown size, total: 64.29 MiB) to /root/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/8d6c32399aa01613d96e2cbc9b13638f359ef62bb33612b077b4c247f6ef99c1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "403468ea5ee34de18009b6ef6645e1f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b71e8cde067443cf9bea20950b3df149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/26.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6839ca3e3082455b99ef7854f402122f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/29.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26b290ff3c994ca88257a3c7a47bbd8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/4.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "988dec1655c943499283d600009008a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/5.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8613db4685c4b24931ccc0c82698174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/60000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset fashion_mnist downloaded and prepared to /root/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/8d6c32399aa01613d96e2cbc9b13638f359ef62bb33612b077b4c247f6ef99c1. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10f633457e574417b5ef7d68362fc75d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To actually download the dataset\n",
    "# dsd creates a datasets dictionary\n",
    "\n",
    "dsd = load_dataset(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9052920a-cddc-47a0-a397-6faa4186c71b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 60000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsd\n",
    "#Note that this returns dictionaries rather than the tuples we previously used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33416e29-2fab-4e0d-9df0-2d9cc5241da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>,\n",
       " 'label': 9}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = dsd['train'], dsd['test']\n",
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f47490e-be17-48d1-adbe-80f0ac795b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('image', 'label')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting x,y as shorthand for images and labels, so we don't have to type that out everytime we use them\n",
    "x,y = ds_builder.info.features\n",
    "x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "438a7e5f-9fc1-4c96-8e7d-2b0fb2b82d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABY0lEQVR4nG3OO0sDQRQF4HNmduMjGBWirg8wICgiCmKjgjaChVhaibVg4S/xV9jZ2FkpdtZpAoqCTyIKGh/RaJKZ3WuVCDO51eV8nDsDuDPkJcBp/iQHYOjy4SzdyILGovtGCl+HW7r60VVpZKqxlJLST7RbSN51CR7eBHGNuOu2YaePF1oq9Rn8Mig3sflm0arwKf/1qPnp41OdSfU8VJ9t9M++5FA1AQxj4zefoUIFCXTw7iNqEEiSxPz/UPMsEkBIoVRaNBVADSVxf4smQSphYHMtURGEjlvgeCohFEmb9XGyaAhNKDOw6OGKKIICLdc7Hs4bTWhAtdcWPMxZJdAC6PRwm4O9WaOFEpOp4985B2dpCdWeUmIngkkH119NEncw7KaN7LSDY11R2kSbo5moJ5OecvCoRunAN2wdcbUAZ6R8cyXLuH27f5NRF6el/CIb+JGyrLoGYGn7YBB7+2v/yR+rVoQPczXmPgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = train[4][x]\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ee4517c-d7e3-49fd-8cbe-a4c11bbc7c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassLabel(num_classes=10, names=['T - shirt / top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'], id=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featy = train.features[y]\n",
    "featy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a098361f-481b-4ebe-a97e-ef869161f770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 0, 0, 3, 0]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a small batch for testing\n",
    "xb = train[:5][x]\n",
    "yb = train[:5][y]\n",
    "yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3f91eeb-f877-4690-bd7c-fa5db5b558bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ankle boot',\n",
       " 'T - shirt / top',\n",
       " 'T - shirt / top',\n",
       " 'Dress',\n",
       " 'T - shirt / top']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# featy is now a Hugging Face class that allows us to convert the integer values to their actual string descriptions, like so:\n",
    "featy.int2str(yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bfffd3-d26c-45b9-b289-1c11be95c7e0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Collating the data so our model can use them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700cc881-e33c-4134-837f-50273768b317",
   "metadata": {},
   "source": [
    "We can define a collate function that returns a dictionary of stacked tensors, \n",
    "this was manually created as Pytorch usually collates with tuples, and Hugginface uses dict.\n",
    "Pytorch is happy with you returning a dictionary for this purpose.\n",
    "\n",
    "Here we also to all the work of the data aug in the collate function, so as to leave the model less abstracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc512cfe-f9e8-4e8b-9d3d-57982c949ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(b):\n",
    "    return {x:torch.stack([TF.to_tensor(o[x]) for o in b]),\n",
    "            y:tensor([o[y] for o in b])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77caa940-9a80-4aab-892f-0b7e514a7313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 1, 28, 28]),\n",
       " tensor([9, 0, 0, 3, 0, 2, 7, 2, 5, 5, 0, 9, 5, 5, 7, 9]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl =DataLoader(train, collate_fn=collate_fn, batch_size=16)\n",
    "b = next(iter(dl))\n",
    "b[x].shape, b[y]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0914c6f6-f542-4177-954e-b14a6b3289c5",
   "metadata": {},
   "source": [
    "We can define a transforms function, where we can then also add data augmentation to the \n",
    "model at the level of the dataloader feeding the model. Here we create a transformed dataset,\n",
    "'tds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c9a6d62-fb24-4dc8-b4fc-e3209f972f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms(b):\n",
    "    b[x] = [TF.to_tensor(o) for o in b[x]]\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0e05aba-0f0d-4474-a58a-24d10ac7fbf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 1, 28, 28]),\n",
       " tensor([9, 0, 0, 3, 0, 2, 7, 2, 5, 5, 0, 9, 5, 5, 7, 9]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With this \"with_transforms\" attribute, transforms is called on the fly, every time __getitem__ is called\n",
    "# This dataset, otherwise, has an API identical to our previous datasets\n",
    "# Additionally, we've got a dictionary of tensors, so we don't need a collate function anymore\n",
    "\n",
    "tds = train.with_transform(transforms)\n",
    "dl = DataLoader(tds, batch_size=16)\n",
    "b = next(iter(dl))\n",
    "b[x].shape, b[y]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f1e932-542a-46d2-ada1-ba0d12a687b9",
   "metadata": {},
   "source": [
    "So having to return the b seems clunky, aesthetically, so we want to simplify the code,\n",
    "doing this in place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43ba748c-be51-443e-b47a-c091332051ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _transformi(b): b[x] = [TF.to_tensor(o) for o in b[x]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c94cc34a-939d-4fae-aac1-3ba6fb2afe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a function-generating function that modifies my inplace function to return something\n",
    "\n",
    "def inplace(f):\n",
    "    def _f(b):\n",
    "        f(b)\n",
    "        return b\n",
    "    return _f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "894b7451-0ddb-4ee5-8893-6306e8db2447",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformi = inplace(_transformi) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e221afda-b9f1-42ce-9633-a6e104568976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 28, 28]), 9)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = train.with_transform(transformi)[0]\n",
    "r[x].shape,r[y]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf3aee97-e4d8-4d73-93fc-cf2f85fe02bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As this is very common in Python, we can use this decorator\n",
    "# It takes transformi, sends it to inplace and then replaces it with the result\n",
    "\n",
    "@inplace\n",
    "def transformi(b): b[x] = [torch.flatten(TF.to_tensor(o)) for o in b[x]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57f71e34-1580-42bb-9d28-e41b2a4fb982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784]), 9)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdsf = train.with_transform(transformi)\n",
    "r = tdsf[0]\n",
    "r[x].shape,r[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f60ac075-a151-4117-8044-2f9af6893b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sidenote on some dictionary functionality, we can use itemgetterf\n",
    "d = dict(a=1,b=2,c=3)\n",
    "ig = itemgetter('a','c')\n",
    "ig(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c6d423f4-58d1-4ae8-9745-f689836a53ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It doesn't even matter if it is really a dictionary,as long as it looks like one\n",
    "\n",
    "class D:\n",
    "    def __getitem__(self, k): return 1 if k=='a' else 2 if k=='b' else 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e109eab0-c65d-46e5-b76e-337d999e0ecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = D()\n",
    "ig(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e76385d2-c619-45e7-bf3a-c3c6a56edf51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['image', 'label']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tdsf.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8fb7d4-2cd9-4079-aff3-c55c6ba576df",
   "metadata": {},
   "source": [
    "Pytorch comes with a default collate function that can take dictionaries, look up the keys in the, \n",
    "and collate all the entries matching the keys. This also works on tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "faf53995-9939-48e0-974c-4e9bad4550f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': [tensor([1, 3])], 'b': [tensor([2, 4])]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = dict(a=[1],b=[2]), dict(a=[3],b=[4])\n",
    "default_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "539ec7de-42f0-4808-b4a9-8edab8bd4570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So now we can create a function that collates our dictionary data into a tuple, as Hugging Face data is \n",
    "# in a dictionary, but Pytorch prefers tuples\n",
    "\n",
    "def collate_dict(ds):\n",
    "    get = itemgetter(*ds.features)\n",
    "    def _f(b): return get(default_collate(b))\n",
    "    return _f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a654dfda-e049-431f-8a6a-b624a5077b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 784]), tensor([9, 0, 0, 3]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlf = DataLoader(tdsf, batch_size=4, collate_fn=collate_dict(tdsf))\n",
    "xb, yb = next(iter(dlf))\n",
    "xb.shape, yb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785f6141-4c7a-4f16-abcf-7cb986898b0f",
   "metadata": {},
   "source": [
    "The data now looks like the data we are used to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81153881-cd53-4c06-b433-663254851dce",
   "metadata": {},
   "source": [
    "## Plotting images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9906ee-6751-4561-96e4-4c30e1bd3fd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
